---
title: "Mini-data-analysis-m3"
author: "William Laplante"
date: "20/10/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Setup

```{r}
library(datateachr)
library(plyr)
library(tidyverse)
library(DiscriMiner)
library(corrr)
library(forcats)
library(broom)
library(here)
```

In milestone 2, we chose two research questions:

1) How does the area of a cancer affects the diagnosis?

2) What is the variable in this dataset that is the best indicator of a malignant cancer?


## Exercise 1 : Special Data Types

#### Task 1

Here, we take the correlation matrix computed in the previous milestone, and we divide into categories the correlation values for the diagnosis column. We then reorder categories using fct_infreq() and plot the bar graph with this new reordering. This is helpful to visualize which category is dominant in our data and which category is the least present. Clearly, we see that the category that is the least present is "very high", meaning most of our variables don't have a strong correlation with the diagnosis. Also the correlation category that is most present is "medium".
```{r}
#we compute the correlation matrix for the cancer_sample dataset. 
cancer_sample_numeric <- cancer_sample %>% replace(cancer_sample=="M","1") %>% replace(cancer_sample=="B","0") %>% transform(diagnosis=as.numeric(diagnosis))
cancer_sample_cormatrix = cor(cancer_sample_numeric)

#we store as a dataframe the correlation matrix
cormatrix_df <- as_cordf(cancer_sample_cormatrix)

#we break the correlation values into categories for the diagnosis column.
category_corr <- cormatrix_df %>% mutate(category=cut(diagnosis, breaks=c(-Inf, 0.2, 0.4, 0.6, 0.75, Inf), labels=c("Very Low", "Low", "Medium", "High", "Very High"))) %>% select(term, diagnosis, category) %>% filter(!is.na(diagnosis)) %>% rename(variable=term, corr_with_diagnosis=diagnosis)

#now we plot the bar graph for the correlation categories in order using the forcats package.
ggplot(category_corr, aes(x = fct_infreq(category))) + geom_bar() + coord_flip()
```

#### Task 2

We now regroup our factor levels into two categories ; one will contain the variables with a "very high" correlation with the diagnosis and the other category will be labeled "other" and contain the remaining variables that won't be used for research purposes. This is useful to show how many variables will be discarded vs how many will be kept for future analyses.

```{r}
ggplot(category_corr, aes(x = fct_lump(category, -1))) + geom_bar()
```


## Exercise 2 : Modelling

#### 2.0
We now choose our final research question and define the variable of interest. Note that here, we change a bit the initial research question, since the original question can already be answered through our previous analysis. 

Research question : If we create two models, one trained with the variables that have "Very High" correlation with the diagnosis, and another with the variables falling under the "other" category, how do these two models compare? 

Variable of Interest : Accuracy of model (based on the diagnosis variable)

#### 2.1

Now, we extract the variables from the "very high" and "other" categories :
```{r}
Other_df <- category_corr %>% filter(category != "Very High") 

Very_High_df <- category_corr %>% filter(category=="Very High")
```

We then make two dataframes containing the data for these two variables. We also convert the column diagnosis to numerical by mapping "M" to 1 and "B" to 0.
```{r}
X_other <- cancer_sample %>% replace(cancer_sample=="M", "1") %>% replace(cancer_sample=="B", "1") %>% transform(diagnosis=as.numeric(diagnosis)) %>% select(Other_df$variable, diagnosis) %>% select(-ID)    

X_very_high <- cancer_sample %>% replace(cancer_sample=="M", "1") %>% replace(cancer_sample=="B", "1") %>% transform(diagnosis=as.numeric(diagnosis)) %>% select(Very_High_df$variable, diagnosis) 
```

Finally, we use tidyverse's lm() function to perform multivariate linear regression using both datasets as inputs (data with "very high" correlation and "other" correlation). Both models output a value between 0 and 1, which can be interpreted as the likelihood of a cancer being malignant.
```{r}
model_other = lm(diagnosis~.,X_other)
model_very_high = lm(diagnosis~., X_very_high)
```

#### 2.2

We now look at the standard error, statistic, and p value of our variables in both models. Then, we assess the accuracy of our models by looking at the adjusted R-squared.
```{r}
tidy(model_other)
```

```{r}
tidy(model_very_high)
```

```{r}
summary(model_other)$adj.r.squared
```
```{r}
summary(model_very_high)$adj.r.squared
```

These adjusted R squared values demonstrate that our model aren't so good, and that we should probably look into non-linear models for this kind of problem.


## Exercise 3 : Reading and writing data


#### 3.1

We take a summary table from milestone 2, and write it as a csv file in the output folder. To do so, we use the here() function.
```{r}
category_corr #this is the summary table from milestone 2
```
```{r}
write.csv(category_corr, here("output", "category_corr.csv")) #we write to the output folder the category_corr dataframe in a csv format.
```

Robustness : This code should work even if we move our mini data analysis folder in some other location on my computer, since the here() function will always make it so we have access to the current path where the project folder is located.

Reproducibility : Deleting the csv file and knitting this Rmd file remakes the csv file in the output folder.

#### 3.2

We now save our two models in R binary files in the output folder. We use the saveRDS() function to save our file, and we use the here() function for the robustness and reproducibility criterias explained above.

```{r}
saveRDS(model_other, here("output", "model_other.rds"))
saveRDS(model_very_high, here("output", "model_very_high.rds"))
```

We check that we can load our models properly and that everything works fine :
```{r}
tidy(readRDS(here("output", "model_very_high.rds")))
```

```{r}
tidy(readRDS(here("output", "model_other.rds")))
```

We see that we get the same models as before. Therefore, we can successfully retrieve our models if needed using the readRDS() function.
